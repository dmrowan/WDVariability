#!/usr/bin/env python
from __future__ import print_function, division
import os, sys
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import argparse
from astropy import log
from os import path
from glob import glob
from subprocess import check_call
import shutil
from astropy.table import Table
import sys
import pylab
import astropy
from radec_conversion import *
import pandas as pd
from astropy.stats import LombScargle
import heapq
import matplotlib.image as mpimg
from scipy.optimize import curve_fit
import subprocess
#Dom Rowan REU 2018

desc="""
This produces the ranked value C for a single source dependent on exposure, periodicity, autocorrelation, and other statistical measures
"""

parser = argparse.ArgumentParser(description=desc)
parser.add_argument("--source", help = "Input source name", default=None, type=str)
parser.add_argument("--csvname", help= "Input full csv file", default=None, type=str)
parser.add_argument("--fap", help = "False alarm probability theshold for periodogram", default=.05, type=float)
parser.add_argument("--prange", help = "Frequency range for identifying regions in periodogram due to expt and detrad", default=.0005, type=float)
parser.add_argument("--w_pgram", help = "Weight for periodogram", default = 1, type=float)
parser.add_argument("--w_expt", help= "Weight for exposure time", default = .25, type=float)
parser.add_argument("--w_ac", help="Weight for autocorrelation", default = 0, type=float)
parser.add_argument("--comment", help="Add comments/interactive mode", default=False, action='store_true')
parser.add_argument("--w_mag", help= "Weight for magnitude", default=.5, type=float)
args= parser.parse_args()

#Define fit function for covariance (no longer used)
def fitfunc(x,a,b,c,d):
    output=[]
    for val in x:
        output.append(a*np.cos(b*val - c)*np.exp(-d*val))
    return(output)


#Load csv tables into pandas df's
if args.source is not None:
    csvpath = "/home/dmrowan/WhiteDwarfs/GALEXphot/LCs/"
    datadic = {}
    source = args.source
    if os.path.isfile(csvpath+str(source)+"-FUV.csv"):
            alldata_FUV = pd.read_csv(csvpath+str(source)+"-FUV.csv")
            datadic['FUV'] = 'exists'
            print("FUV data exists for " + str(source))
    else:
            datadic['FUV'] = None

    if os.path.isfile(csvpath+str(source)+"-NUV.csv"):
            alldata_NUV = pd.read_csv(csvpath+str(source)+"-NUV.csv")
            datadic['NUV'] = 'exists'
            print("NUV data exists for " + str(source))
    else:
            datadic['NUV'] = None

elif args.csvname is not None:
    datadic = {}
    csvpath = args.csvname
    for i in range(len(csvpath)):
        character = csvpath[i]
        if character == 'c':
            endidx=i-5
            break

    source = csvpath[0:endidx]

    if os.path.isfile(csvpath):
        if csvpath[-7] == 'N':
            datadic['NUV'] = 'exists'
            datadic['FUV'] = None
            alldata_NUV = pd.read_csv(csvpath)
        elif csvpath[-7] == 'V':
            datadic['FUV'] = 'exists'
            datadic['NUV'] = None
            alldata_FUV = pd.read_csv(csvpath)
        else:
            datadic['FUV'] = None
            datadic['NUV'] = None

#Check that we have data
if datadic['FUV'] is None and datadic['NUV'] is None:
    print("No data or incorrect source name")
    sys.exit()
 
#Do everything for NUV first
if datadic['NUV'] is not None:
    #Find the flux of the star
    #averageflux_lambda = np.mean(alldata_NUV['flux_bgsub'])
    #averageflux_nu = 3.34e4 * (1542.26)**2 * averageflux_lambda
    #m_ab = -2.5*np.log10(averageflux_nu / 3631)
    averagemag = np.mean(alldata_NUV['mag_bgsub'])
    m_ab = averagemag

    #Calculate c_mag based on ranges:
    if m_ab < 16:
        c_mag = 1
    elif m_ab < 17:
        c_mag = .5
    elif m_ab < 18:
        c_mag = .25
    else:
        c_mag = 0

    #Break the table into time range groups
    breaks_NUV = []
    for i in range(len(alldata_NUV['t0'])):
        if i != 0:
            if (alldata_NUV['t0'][i] - alldata_NUV['t0'][i-1]) >= 100:
                breaks_NUV.append(i)

    data_NUV = np.split(alldata_NUV, breaks_NUV)
    print("Dividing NUV data for source " + source+ " into "+str(len(data_NUV))+" exposure groups")
    #Each range group has C calculated seperately. We can then choose the most interesting one to plot in our quick-look plot
    df_number = 1
    c_vals = []
    df_numbers_run = []
    biglc_time = []
    biglc_counts = []
    biglc_err = []
    strongest_periods_list = []
    for df in data_NUV:
        #Find exposure time
        #Question: using t1 and t0 columns, or just t0
        lasttime = list(df['t1'])[-1]
        firsttime = list(df['t0'])[0]
        exposure = lasttime - firsttime
        c_exposure = (lasttime - firsttime) / 1000

        #Reset first time in t_mean to be 0
        firsttime_mean = df['t_mean'][df.index[0]]
        df['t_mean'] = df['t_mean'] - firsttime_mean

        #Find indicies of data above 5 sigma of mean (counts per second column), flagged points, and pointes with
        # less than 10 seconds of exposure time
        stdev = np.std(df['cps_bgsub'])
        redpoints = np.where( ((df['cps_bgsub'] - np.mean(df['cps_bgsub'])) > 5*stdev) | (df['flags']!=0) | (df['exptime'] < 10) )[0]
        redpoints = redpoints + df.index[0]
        #print("Removing " +  str(len(redpoints)) + " bad points")
        df_reduced = df.drop(index=redpoints)
        

        if df_reduced.shape[0] < 7:
            print("Not enough points for this exposure group, skipping. Removed " +  str(len(redpoints)) + " bad points")
            df_number +=1
            continue

        #If first point is not within 3 sigma, remove
        if (df_reduced['cps_bgsub'][df_reduced.index[0]] - np.mean(df['cps_bgsub'])) > 3*stdev:
            df_reduced = df_reduced.drop(index=df_reduced.index[0])

        #If last point is not within 3 sigma, remove
        if (df_reduced['cps_bgsub'][df_reduced.index[-1]] - np.mean(df['cps_bgsub'])) > 3*stdev:
            df_reduced = df_reduced.drop(index=df_reduced.index[-1])

        #Get the cps_bgsub, error and time columns and make correction for relative scales
        cps_bgsub = df_reduced['cps_bgsub']
        cps_bgsub_median = np.median(cps_bgsub)
        cps_bgsub = ( cps_bgsub / cps_bgsub_median ) - 1.0
        cps_bgsub_err = df_reduced['cps_bgsub_err'] / cps_bgsub_median
        t_mean = df_reduced['t_mean']

        #Make the correction for relative scales for redpoints
        if len(redpoints) != 0:
            cps_bgsub_red = df['cps_bgsub'][redpoints]
            cps_bgsub_median_red = np.median(cps_bgsub_red)
            cps_bgsub_red = (cps_bgsub_red / cps_bgsub_median_red) - 1.0
            cps_bgsub_err_red = df['cps_bgsub_err'][redpoints] / cps_bgsub_median_red
            t_mean_red = df['t_mean'][redpoints]

        ###Periodogram Creation###
        #Fist do the periodogram of the data
        ls = LombScargle(t_mean, cps_bgsub)
        freq, amp = ls.autopower(nyquist_factor=1)
        
        #Periodogram for dither information
        ls_detrad = LombScargle(df_reduced['t_mean'], df_reduced['detrad'])
        freq_detrad, amp_detrad = ls_detrad.autopower(nyquist_factor=1)

        #Identify statistically significant peaks
        top5amp = heapq.nlargest(5, amp)
        #top5amp_expt = heapq.nlargest(5, amp_expt)
        top5amp_detrad = heapq.nlargest(5, amp_detrad)
        #Find bad peaks
        bad_detrad = []
        for a in top5amp_detrad:
            idx = np.where(amp_detrad == a)[0]
            f = freq[idx]
            lowerbound = f - args.prange
            upperbound = f + args.prange
            bad_detrad.append( (lowerbound, upperbound) )

        #Calculate false alamr thresholds for ars.fap
        probabilities = [args.fap]
        faplevels = ls.false_alarm_level(probabilities)

        sspeaks = [] #freq,amp,fap tuples
        for a in top5amp:
            #False alarm probability threshold
            fap = ls.false_alarm_probability(a)
            if fap <= args.fap:
                ratio = a / ls.false_alarm_level(args.fap)
                idx = np.where(amp==a)[0]
                f = freq[idx]
                #Now check if it is in any of the bad ranges
                hits = 0
                for tup in bad_detrad:
                    if ( f > tup[0] ) and ( f < tup[1] ):
                        hits+=1

                #If hits is still 0, the peak isnt in any of the bad ranges
                if hits == 0:
                    sspeaks.append( (f, a, fap, ratio) ) 
        
        c_periodogram = 0
        for peak in sspeaks:
            if len(sspeaks) < 3:
                c_periodogram += peak[3]
            if len(sspeaks) >= 3:
                c_periodogram += peak[3] * .25

        #Grab the info to show the strongest peak for the source
        if len(sspeaks) != 0:
            sspeaks_amp = [ peak[1] for peak in sspeaks ] 
            sspeaks_freq = [ peak[0] for peak in sspeaks ] 
            sspeaks_ratio = [ peak[3] for peak in sspeaks ]
            strongest_freq = (sspeaks_freq[np.where(np.asarray(sspeaks_amp) == max(sspeaks_amp))[0][0]])
            strongest_period_ratio = (sspeaks_ratio[np.where(np.asarray(sspeaks_amp)==max(sspeaks_amp))[0][0]])
            strongest_period = 1.0 / strongest_freq
            strongest_periods_list.append((strongest_period[0], strongest_period_ratio))

        ###Autocorrelation results###
        autocorr_result = np.correlate(cps_bgsub, cps_bgsub, mode='full')
        autocorr_result = autocorr_result[int(autocorr_result.size/2):]
            
        if any(np.isinf(x) for x in autocorr_result):
            print("Infinite Values in Autocorr for group "+str(df_number))
            #Reassign Autocorr_result to be a bunch of zeros
            numberofzeros = len(autocorr_result)
            autocorr_result = np.zeros(numberofzeros)

        ac_x = range(len(autocorr_result))
        '''
        popt, pcov = curve_fit(fitfunc, ac_x, autocorr_result)
        '''
        params = np.polyfit(ac_x, autocorr_result, 1)
        residuals = autocorr_result - (ac_x*params[0]+params[1])
        #residuals = autocorr_result - fitfunc(ac_x, *popt)

        ss_res = np.sum(residuals**2)
        ss_tot = np.sum((autocorr_result-np.mean(autocorr_result))**2)
        if ss_tot == 0:
            r_squared = 0
            c_autocorr = 0
        else:
            r_squared = 1 - (ss_res / ss_tot)
            c_autocorr = 1 - r_squared


        ###Generate rating###
        C = (args.w_pgram * c_periodogram) + (args.w_expt * c_exposure) + (args.w_ac * c_autocorr) + (args.w_mag * c_mag)
        print("Exposure group "+str(df_number)+" ranking: "+ str(C))
        c_vals.append(C)


        ###Generate plot/subplot information###
        fig, ax = plt.subplots(2,2,figsize=(16,12))
        fig.suptitle(source+"-NUV \n" + 
                "AB Magnitude: " + str(round(m_ab, 2)) + ", Exposure: "+str(exposure)+ " seconds \n" +
                "Exposure group: " + str(df_number) +"\n"+
                #"SSPs: "+str(len(sspeaks)) + "\n" +
                "Periodogram ratio: "+str(c_periodogram) + "\n" +
                "Autocorr Fit R^2: " + str(r_squared) + "\n" +
                "Ranking: " + str(C)
                )

        #Subplot for LC
        #Change: Divide cps by median then subtract 1 from it, use this for periodogram and lc plota
        #Change: Divide error column by median cps as well
        ax[0][0].errorbar(t_mean, cps_bgsub, yerr=cps_bgsub_err, color='purple', marker='.', ls='-')
        if len(redpoints) != 0:
            ax[0][0].errorbar(t_mean_red, cps_bgsub_red, yerr=cps_bgsub_err_red, color='r', marker='.')
        
        ax[0][0].set_title('NUV light curve')
        ax[0][0].set_xlabel('Mean Time (GALEX)')
        ax[0][0].set_ylabel('Variation in CPS')

        #Subplot for autocorr
        ax[1][0].plot(autocorr_result, 'b-', label='data')
        #ax[1][0].plot(ac_x, fitfunc(ac_x, *popt), 'g-', label='fit')
        ax[1][0].plot(residuals, 'r--', alpha=.25, label='residuals')
        ax[1][0].plot(ac_x, ac_x*params[0]+params[1], 'r--', alpha=.5, label='linear fit')
        ax[1][0].set_title('Autocorrelation')
        ax[1][0].set_xlabel('Delay')
        ax[1][0].legend()

        #Subplot for periodogram
        ax[0][1].plot(freq, amp, 'g-', label='Data')
        ax[0][1].plot(freq_detrad, amp_detrad, 'r-', label="Detrad", alpha=.25)
        ax[0][1].set_title('NUV Periodogram')
        ax[0][1].set_xlabel('Freq [Hz]')
        ax[0][1].set_ylabel('Amplitude')
        ax[0][1].set_xlim(0, np.max(freq))
        if any(np.isnan(x) for x in top5amp_detrad):
            print("No detrad peaks")
        else:
            for tup in bad_detrad:
                ax[0][1].axvspan(tup[0], tup[1], alpha=.1, color='black')
        
        #ax[0][1].axvline(x=nyquistfreq, color='r', ls='--')
        for level in faplevels:
            idx = np.where(level == faplevels)[0][0]
            fap = probabilities[idx]
            ax[0][1].axhline(level, color='black', alpha = .5, ls = '--', label = 'FAP: '+str(fap))

        ax[0][1].legend()
        #Subplot for png image
        pngfile = "/home/dmrowan/WhiteDwarfs/GALEXphot/pngs/"+source+".png"
        img1 = mpimg.imread(pngfile)
        ax[1][1].imshow(img1)
        #Turn of axes 
        ax[1][1].axis('off')

        saveimagepath = str("PNGs/"+source+"qlp"+str(df_number)+".png")
        fig.savefig(saveimagepath)
        df_numbers_run.append(df_number)
        df_number += 1
        
        #Information for big light curve
        biglc_time.append(np.mean(t_mean + firsttime_mean))
        biglc_counts.append(np.mean(cps_bgsub))
        biglc_err.append(np.std(cps_bgsub_err) / np.sqrt(df_reduced.shape[0]))

#Find the total rank, best rank, and best group
totalrank = np.sum(c_vals)
if len(c_vals) !=0:
    bestrank = max(c_vals)
    idx_best = np.where(c_vals == bestrank)[0][0]
    best_expt_group = df_numbers_run[idx_best]
else:
    bestrank = 0
    idx_best = 0
    best_expt_group=0
print(source, "Total rank: " + str(totalrank), "Best rank: " + str(bestrank), "Best group: " + str(best_expt_group))

###Commenting/Interactive Mode###
if args.comment:
    if bestrank >= 0:
        bestimagepath = "PNGs/"+source+"qlp"+str(best_expt_group)+".png"
        subprocess.call(['display', bestimagepath])
        comment = input("Message code: ")
else:
    comment=""

###Get most prevalent period from strongest_periods_list###
all_periods = [ tup[0] for tup in strongest_periods_list ]
all_ratios = [ tup[1] for tup in strongest_periods_list ] 
if len(all_periods) > 1:
    period_to_save = all_periods[np.where(np.asarray(all_ratios) == max(all_ratios))[0][0]]
elif len(all_periods) == 1:
    period_to_save = all_periods[0]
    period_to_save = round(period_to_save,3)
else:
    period_to_save = ''

###Query Catalog###
#If catalog to query does not exist, make one
if not os.path.isfile("AllCatalog_Simbad.csv"):
    print("Creating catalog")
    subprocess.run(["WDsearch"])

#Read in Simbad catalog
df_simbad = pd.read_csv("AllCatalog_Simbad.csv")
idx_simbad = np.where(df_simbad["SourceName"] == csvpath[:-8])[0]
simbad_name = df_simbad["SimbadName"][idx_simbad]
#Read in SDSS info
df_sdss = pd.read_csv("/home/dmrowan/WhiteDwarfs/Catalogs/SDSSCatalog.csv")
idx_sdss = np.where(df_sdss['SDSS-J'] == csvpath[:-8])[0]
if len(idx_sdss) != 0:
    idx_sdss = idx_sdss[0]
    g_mag = df_sdss['g'][idx_sdss]
    sdss_dtype = df_sdss['dtype'][idx_sdss]
else:
    g_mag = ""
    sdss_dtype = ""

#Generate output csv with pandas
dfoutput = pd.DataFrame()
dfoutput = dfoutput.append({"SourceName":source, "TotalRank":round(totalrank,3), "BestRank":round(bestrank,3), "Comment":comment, "ABMag":round(m_ab, 2), "StrongestPeriod":period_to_save, "SimbadName":simbad_name, "gmag":g_mag, "dType":sdss_dtype}, ignore_index=True)
dfoutput.to_csv("Output/"+source+"-NUV-output.csv")


###Generate multiplage pdf###
if datadic['NUV'] is not None:
    #Make the correction for relative scales
    alldata_tmean = alldata_NUV['t_mean']
    alldata_cps_bgsub = alldata_NUV['cps_bgsub']
    alldata_mediancps = np.median(alldata_cps_bgsub)
    alldata_cps_bgsub = ( alldata_cps_bgsub / alldata_mediancps ) - 1.0
    alldata_cps_bgsub_err = alldata_NUV['cps_bgsub_err'] / alldata_mediancps
    figall, axall = plt.subplots(1,1, figsize=(16,12))
    figall.suptitle("Combined Light Curve for " + source + "NUV + \n"+
            "AB magnitude " + str(round(m_ab, 2)) + "\n" +
            "Total rank: " + str(round(totalrank,2)) + " in "+str(len(data_NUV))+ " exposure groups \n"+
            "Best rank: " +str(round(bestrank,2))+ " in exposure group " + str(best_expt_group) +"\n"+
            "SDSS Type: " + str(sdss_dtype) + " g mag: " + str(g_mag) + "\n"
            )
    axall.errorbar(biglc_time, biglc_counts, yerr=biglc_err, color='purple', marker='.', ls='-',  zorder=2, ms=15)
    axall.errorbar(alldata_tmean, alldata_cps_bgsub, yerr=alldata_cps_bgsub_err, color='black', marker='.', zorder=1, ls='', alpha=.125)
    allsaveimagepath = str("PNGs/"+source+"all-NUV.png")
    figall.savefig(allsaveimagepath)
#Call the pdfcreator script
subprocess.call(['PDFcreator', '-s', source])



